\chapter{Discussion}
\section{Overview}
The following section will discuss the results gained during the study that have been presented in Chapter 4 and Chapter 5, Participant Demographics and Research Results respectively. They will be discussed in the context of this projects aim, objectives and research questions that were presented in Chapter 1. The results will also be compared with previous studies as discussed within the literature review, especially those regarding the works investigating the roles of previous user experience, such as that of Dillon and Song's work with graphical element prompts and how Novice users interacted with them \citep*{dillon1997empirical}.

\section{Results Discussion}

\subsection{Issue and Error Detection}
The main aim of this project was to assess the impact that previous user experience has upon the Think-aloud methodology, within a concurrent testing environment. Therefore as part of this assessment between Novice and Experienced users I have collected extensive data on the amount and specific types of issues encountered by each type of participant, other metrics will be assessed in the sections following this one. Research Question One (RQ1) and Two (RQ2) can now be answered following this data collection and analysis. Types of errors encountered by both types of participant show overlap when regarding each experience sample as a collective. This was especially seen with Task C and D, as both types of users discovered the same exact issues, namely not being able to access required interface elements when attempting Task D, as the proprieties menu for the tested game ``Hollow Knight" was not obvious to the user regardless of their experience for the majority of participants. These results confirm other studies results using different programs, regarding the overlap of error types \citep{prumper1991errors}, which found Novice and Expert participants users to find the same types of errors whilst using a set of standard office tools. However, this research differs from previous studies as well, as in this study Novice and Expert found a different amount of issues and errors overall, with Novice users finding more issues compared with the Expert users, with thirty total issues detected compared with only eleven for Experienced participants. This difference has also been present in previous studies \citep{gerardo2007effectiveness}. Therefore with regards to Issues and Errors, the data in this study presents the fact that Novice and Experienced users show similarities in the errors that are detected, but also a great difference in the total number of errors detected overall, and the null hypothesis introduced in Chapter 1, must at least be partially rejected. Additionally, by having a smaller sample or less diverse sample size this would likely have led to a smaller amount of detected errors overall.

\subsection{Task Duration}
The next metric for discussion is the time it took both types of participant to complete each of the Tasks, known in this study as ``Task Duration" and was collected for each Task individually. Therefore Research Question Three (RQ3) can now be analysed. As highlighted in Chapter 5, some of the six tasks showed significant differences between Novice and Experienced users. Task C had the most time spent for both Novice and Experienced users, which was expected as Task C was designed to be the most complex Task. Novice, Experienced and combined user data sets show a great difference in variation between the different Tasks (see Table 5.6). Experienced users had a tighter grouping of variance amongst the sixteen participants, as compared with Novices which showed greater variance, that is with the exception of Task C, as both groups share a significant level of variation in that Task. Task D showed less variation for Novice results compared with the Experienced participants, meaning that more Novice users took a similar amount of time (which was longer than that of Experienced users). As presented in Table 5.7, Kruskal-Wallis examinations show that Task A, B and E show a significant level of difference in Novice and Experienced participants data sets. Therefore half of the Tasks show what a significant difference previous user experience makes upon how long Tasks are completed or attempted. The fact that these Tasks show this significant could be indicative of problems within the Steam client, but crucially shows that Concurrent Think-aloud testing will be impacted by a participants level of experience regarding Task Duration. Novices are likely to take longer to complete tasks at least in 50\% of tested cases. Therefore, regarding Research Question Three, there is the presence of a discrepancy within this study between the tested experience groups.

\subsection{Task Completion}
Task Completion was also assessed within this study, if a Task had a high failure rate, it would be potentially indicative of a problem within the tested service or problems with the description or structure of the study. Some of the tasks show high failure rates within this study, as well as consistent successful Tasks between the types of participant, and as such Research Question Four can now be assessed. Overall Task D was the most difficult Task, for both Novice and Experienced users, holding a failure rate of 75\% when considering all 32 participants in both categories combined, the reason for this was likely twofold. Firstly, this tasks interface hidden from the user, requiring knowledge on how to access games properties menu, and thus is a significant design issue within the Steam interface. Secondly, the word ``Verification" confused many of my participants as they did not understand what that meant in regards to game services, this was especially true of my Novice participants as many would verbally explain their confusion. Therefore this study has uncovered a problem regarding the way in which Tasks a presented to participants, an observation seen in other works prior to this \citep{gerardo2007effectiveness}; but also uncovered the tested service design faults at least 4 Tasks. On an individual level, Task C showed a significant number of Novice participants failing the Task, alongside Task E and F. Whereas Task C was only failed by 5 experienced participants (31.3\%). Task F is shows significant differences between Novice and Experienced groups following the Kruskal-Wallis test. However according to Kruskal-Wallis examinations of the Novice and Experienced data sets, only Task F shows a significant difference of success or fail ratio in Task Completion, with only eight Novice participants successfully completing the Task, compared with all sixteen Experienced participants completing Task F. Therefore Research Question Four can be answered by saying that in the majority of cases experienced users are more likely to have a higher completion success rate, compared with Novice users when tasks are regarding more complex functions. However in the case of Task A, functions which are similar to other services such as database records or shopping websites such as Amazon.com, lead to close or exact similarity between Novice and Experienced participants. There is also no significant difference between the data sets according to the Kruskal-Wallis test, except with Task F.

\subsection{Task Mouse Interactions}
Additionally this study also included the collection of the number of Mouse Interactions per Task which is not commonly seen in other studies similar to this work. Mouse Interactions were defined as how many clicks of the mouse were made whilst attempting or completing each Task. Therefore Research Question Five can now be examined. As with the other metrics recorded in this project, a higher amount of mouse interactions could again be indicative of a design problem within the tested service, as a large amount of clicks could mean that participants are unsure of how the tested interface is supposed to work. For the majority of Tasks, Novice users clicked more times during their attempts as compared with Experienced participants. The former statement is true for the mean Mouse interactions made for each Task except with Task D. Interestingly, Task C shows that Novice and Experienced users were extremely similar in their use of the program regarding Mouse interactions (see Table 5.11). Mouse Interactions also show some variation between Novice and Experienced data sets, again showing that Novice users are more varied in their approach to using Steam, and experienced participants are less so, taking a similar amount of Mouse interactions with all tasks with the exception of Task C, where the inverse of this true and Novice users are more consistent in the numbers of Interactions made in that Task. The Kruskal Wallis examination of Mouse Interactions Data do show some significant differences between the two groups, but only in 50\% of tasks, Task B, E and F, much like with Task Duration which showed significant differences in three Tasks (Tasks A, B and E). Therefore to answer Research Question Five, Novice and Experienced users do show significant differences in the number of Mouse Interactions made in 50\% of tasks, but not for the remainder of tasks.

\subsection{Participant Post Questionnaire}
Lastly, participants were asked for their thoughts on their involvement in this study. Firstly participants were asked for their thoughts on the Steam services usability and secondly their ratings of Thinking-aloud and whether or not that distracted them during the study, and as such Research Question 6 can now be answered. Before the collection of the results, the expectation was that Novice users would find the Steam service more difficult and less user friendly compared to the experienced participants who were used to gaming services from other platforms.  This proved to be the case for the majority of Novice participants as seen in Figure 4.13 as most participants rated the service in the D or E category, elaborating that visual elements were complex, cluttered or obscure, this coupled with problems with understanding aspects of gaming showed both a higher amount of issues (as discussed above) and thus lead to this assessment of the Steam interface in terms of its interface. Conversely the majority of Experienced participants commented on the services relative ease of use and lacking problems in functionality, except when commenting on Task D, as the properties menu for games is hidden and not very intuitive. However when asked about the Think-aloud process their is no significant difference in what each experience group commented on, as both groups responded favourably to Thinking-aloud and that for the most part they did not find it to interfere with there use of the program. Thus to answer Research Question 6, it can be stated that participants react to the Think-Aloud process similarly regardless of previous user knowledge, However they do react differently when asked about the overall usability of the tested service. 


\section{Summary}
This chapter has discussed the results of the study as seen in Chapter 4 (Participant Demographics) and Chapter 5 (Research Results). There have been striking differences between Novice and Experienced participants in these different metrics especially regarding tasks C, D and F, but also some unexpected similarities regarding some Tasks which was not anticipated prior to this study. These results have also been compared with previous works.  







